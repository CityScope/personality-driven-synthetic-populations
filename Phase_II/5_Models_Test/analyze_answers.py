"""
analyze_answers.py

This script compares personality alignment of responses generated by fine-tuned LLaMA 3 models
(Explorer, Diplomat, Analyst, Sentinel) against the base LLaMA 3 model.

For each model:
- Loads its responses and compares them to the base (LLaMA3) on trait-level and full-vector alignment.
- Computes statistical metrics: mean, standard deviation, similarity, MAE, and Euclidean distance.
- Compares generated trait scores to their respective target vector.
- Outputs detailed trait-by-trait and response-by-response comparisons to a .txt report file.

Input:
- JSON files containing responses and Big Five scores for each model in the root directory.

Output:
- One text report file per model (e.g. `Explorer_001.txt`) in `./new_model_test/`

Author: José Miguel Nicolás García
"""

import json
import os
import numpy as np

# Big Five trait names (OCEAN)
traits = ["Openness", "Conscientiousness", "Extraversion", "Agreeableness", "Neuroticism"]

def similarity(vec1, vec2, alpha=5.0):

    """
    Computes the designed exponential similarity between two Big Five vectors.

    Args:
    vec1 (dict): First personality vector.
    vec2 (dict): Second personality vector.
    alpha (float): Penalty sharpness parameter.

    Returns:
    float: Similarity score in range [0, 1]
    """
    penalties = [np.exp(-alpha * (vec1[t] - vec2[t]) ** 2) for t in traits]
    product = np.prod(penalties)
    return float(product ** (1 / len(penalties)))


# Target personality vectors per model type
personality_targets = {
    "Analyst":   {"Openness": 0.9,  "Conscientiousness": 0.6,  "Extraversion": 0.35, "Agreeableness": 0.25, "Neuroticism": 0.35},
    "Diplomat":  {"Openness": 0.9,  "Conscientiousness": 0.5,  "Extraversion": 0.45, "Agreeableness": 0.9,  "Neuroticism": 0.6},
    "Sentinel":  {"Openness": 0.25, "Conscientiousness": 0.9,  "Extraversion": 0.50, "Agreeableness": 0.65, "Neuroticism": 0.40},
    "Explorer":  {"Openness": 0.4,  "Conscientiousness": 0.25, "Extraversion": 0.8,  "Agreeableness": 0.65, "Neuroticism": 0.25},
}

# Models to compare (same keys used in filenames)
model_keys = [  "Analyst", "Diplomat", "Explorer", "Sentinel", 
              ]
# Input and output directories
input_dir = "."
output_dir = "./new_model_test"
os.makedirs(output_dir, exist_ok=True)

# Load base model (LLaMA3) outputs once
with open(os.path.join(input_dir, "generated_responses_LLaMA3.json"), "r", encoding="utf-8") as f:
    llama3_data = json.load(f)

# Process each fine-tuned model
for model_key in model_keys:
    target = personality_targets[model_key]
    file_path = os.path.join(input_dir, f"generated_responses_{model_key}_001-b.json")
    out_path = os.path.join(output_dir, f"{model_key}_001.txt")

    if not os.path.exists(file_path):
        print(f"File not found for model: {model_key}")
        continue

    with open(file_path, "r", encoding="utf-8") as f:
        model_data = json.load(f)

    with open(out_path, "w", encoding="utf-8") as out:
        out.write(f"Results for model: {model_key}\n\n")

        # === Trait-level statistics ===
        scores_by_trait = {trait: [] for trait in traits}
        for entry in model_data:
            if "big_5" in entry and "Score" in entry["big_5"]:
                for trait in traits:
                    score = entry["big_5"]["Score"].get(trait)
                    if isinstance(score, float):
                        scores_by_trait[trait].append(score)

        out.write(f"{'Trait':<18} {'Model μ':>8} {'σ(M)':>8}  {'LLaMA3 μ':>10} {'σ(L3)':>8}  {'Target':>10} {'Δμ':>8} {'✓':>5}\n")
        out.write("-" * 85 + "\n")
        for trait in traits:
            # Trait stats for fine-tuned model
            model_vals = scores_by_trait[trait]
            model_mean = np.mean(model_vals)
            model_std = np.std(model_vals)

            # Trait stats for base model 
            llama_vals = [
                entry["big_5"]["Score"][trait]
                for entry in llama3_data[:len(model_vals)]
                if "big_5" in entry and "Score" in entry["big_5"]
            ]
            llama_mean = np.mean(llama_vals)
            llama_std = np.std(llama_vals)

            # Target
            target_val = personality_targets[model_key][trait]

            # Delta and improvement
            delta = abs(model_mean - llama_mean)
            dist_model = abs(model_mean - target_val)
            dist_llama = abs(llama_mean - target_val)
            improved = dist_model < dist_llama
            checkmark = "✅" if improved else "❌"

            out.write(f"{trait:<18} {model_mean:>8.3f} {model_std:>8.3f}  {llama_mean:>10.3f} {llama_std:>8.3f}  {target_val:>10.2f} {delta:>8.3f} {checkmark:>5}\n")


         
        similarities, maes, euclideans = [], [], []
        base_sim, base_mae, base_eucl = [], [], []

        out.write(f"{f'Response {i+1}':<10} {sim:>8.3f} {mae:>8.3f} {dist:>8.3f}   {sim_b:>10.3f} {mae_b:>10.3f} {dist_b:>10.3f}\n")
        out.write("-" * 70 + "\n")

        for i, (entry_m, entry_l3) in enumerate(zip(model_data, llama3_data)):
            if "big_5" not in entry_m or "Score" not in entry_m["big_5"]:
                continue
            if "big_5" not in entry_l3 or "Score" not in entry_l3["big_5"]:
                continue

            m_score = entry_m["big_5"]["Score"]
            l3_score = entry_l3["big_5"]["Score"]

            # Fine-tuned model metrics
            sim = similarity(m_score, target)
            mae = np.mean([abs(m_score[t] - target[t]) for t in traits])
            dist = np.sqrt(sum((m_score[t] - target[t]) ** 2 for t in traits))
            similarities.append(sim)
            maes.append(mae)
            euclideans.append(dist)

            # Base model metrics
            sim_b = similarity(l3_score, target)
            mae_b = np.mean([abs(l3_score[t] - target[t]) for t in traits])
            dist_b = np.sqrt(sum((l3_score[t] - target[t]) ** 2 for t in traits))
            base_sim.append(sim_b)
            base_mae.append(mae_b)
            base_eucl.append(dist_b)

            out.write(f"{f'Resp. {i+1}':<10} {sim:>8.3f} {mae:>8.3f} {dist:>8.3f}   {sim_b:>10.3f} {mae_b:>10.3f} {dist_b:>10.3f}\n")

        # === Summary metrics ===
        out.write(f"\nAverages:\n")
        out.write(f"{'Metric':<20} {'Model':>10} {'LLaMA3':>10}\n")
        out.write("-" * 42 + "\n")
        out.write(f"{'Average Similarity':<20} {np.mean(similarities):>10.3f} {np.mean(base_sim):>10.3f}\n")
        out.write(f"{'Average MAE':<20} {np.mean(maes):>10.3f} {np.mean(base_mae):>10.3f}\n")
        out.write(f"{'Average Euclidean':<20} {np.mean(euclideans):>10.3f} {np.mean(base_eucl):>10.3f}\n")

    print(f"Results saved to {out_path}")
