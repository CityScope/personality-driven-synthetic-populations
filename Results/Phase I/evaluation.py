"""
evaluation.py

This script analyzes structured personality profiles generated by various LLMs 
(e.g., LLaMA, Dolphin, Mistral, Qwen) by computing:

- Lexical diversity (Distinct-1)
- Cosine similarity via sentence embeddings
- Jaccard similarity per attribute
- Global entropy of profile content
- Most similar and dissimilar profile pairs
- Similarity of personality and general descriptions

The script outputs summary statistics and similarity histograms per model.

Author: José Miguel Nicolás García
"""


import json
import math
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter
import re
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import torch
import sys

# Map of model names to their corresponding JSON input files
model_files = {
    "LLaMA": "profiles_cambridge-llama3.1_8b.json",
    "Dolphin": "profiles_cambridge-dolphin-llama3_8b.json",
    "Mistral": "profiles_cambridge-mistral_7b.json",
    "Qwen": "profiles_cambridge-qwen3_8b.json"
}

# Optional color configuration for plots (currently unused)
model_colors = {
    "LLaMA": "#4E79A7",   # blue
    "Dolphin": "#59A14F",   # green
    "Mistral":  "#F28E2B",   # orange
    "Qwen": "#E15759"       # red
}


def flatten_json(data, parent_key=""):
    """
    Recursively flattens nested JSON into a flat dictionary with dot-separated keys.

    Args:
        data (dict): A nested dictionary representing a profile.
        parent_key (str): Used for recursion to build compound keys.

    Returns:
        dict: Flattened key-value pairs.
    """
    items = []
    for key, value in data.items():
        new_key = f"{parent_key}.{key}" if parent_key else key
        if isinstance(value, dict):
            items.extend(flatten_json(value, new_key).items())
        else:
            items.append((new_key, value))
    return dict(items)

def profile_to_text(profile):
    """
    Converts a structured profile into a single string for embedding or comparison.

    Args:
        profile (dict): Structured profile.

    Returns:
        str: Flattened string version of the profile.
    """
    flat_profile = flatten_json(profile)
    return " | ".join([f"{key}:{value}" for key, value in flat_profile.items()])

def calculate_distinct_1(compact_profiles):
    """
    Calculates Distinct-1 (lexical diversity) score over a list of text profiles.

    Args:
        compact_profiles (List[str]): Profiles in text format.

    Returns:
        float: Distinct-1 score.
    """
    tokens = [word for profile in compact_profiles for word in profile.split()]
    unique_tokens = set(tokens)
    distinct_1 = len(unique_tokens) / len(tokens)
    return distinct_1


def calculate_jaccard_similarity(attribute_values_1, attribute_values_2):
    """
    Calculates Jaccard similarity between two sets of attribute values.

    Args:
        attribute_values_1 (List[str])
        attribute_values_2 (List[str])

    Returns:
        float: Jaccard similarity score.
    """
    set1, set2 = set(attribute_values_1), set(attribute_values_2)
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    return intersection / union if union != 0 else 0

def jaccard_similarity_per_attribute(profiles):
    """
    Computes average Jaccard similarity for each attribute across all profile pairs.

    Args:
        profiles (List[dict]): List of profiles.

    Returns:
        dict: Mapping attribute name → average Jaccard similarity.
    """
    attributes = list(flatten_json(profiles[0]).keys())
    jaccard_results = {attr: [] for attr in attributes}
    
    for i in range(len(profiles)):
        for j in range(i + 1, len(profiles)):
            profile1 = flatten_json(profiles[i])
            profile2 = flatten_json(profiles[j])
            for attr in attributes:
                if attr in profile1 and attr in profile2:
                    similarity = calculate_jaccard_similarity(
                        str(profile1[attr]).split(), str(profile2[attr]).split()
                    )
                    jaccard_results[attr].append(similarity)

    jaccard_avg = {attr: sum(vals) / len(vals) if vals else 0 for attr, vals in jaccard_results.items()}
    return jaccard_avg


def calculate_global_entropy(profiles):
    """
    Computes the global entropy over all values in all profiles.

    Args:
        profiles (List[dict]): List of structured profiles.

    Returns:
        float: Global entropy score.
    """
    all_values = [str(value) for profile in profiles for value in flatten_json(profile).values()]
    freq = Counter(all_values)
    total = sum(freq.values())
    entropy = -sum((count / total) * math.log2(count / total) for count in freq.values())
    return entropy


# Process each model and generate outputs
for model_name, file_path in model_files.items():
    print(model_name)
    # Redirect stdout to output file for logging
    output_prefix = f"output_{model_name}"
    sys.stdout = open(f"{output_prefix}.txt", 'w')

    
    # Load model profiles 
    with open(file_path, "r") as file:
        profiles = json.load(file)
    
    compact_profiles = [profile_to_text(profile) for profile in profiles]
    # Extract general and Big Five personality descriptions
    descriptions = [profile["General"].get("General Description", "") for profile in profiles]
    descriptions = list(filter(lambda x: x != "", descriptions))

    # === Profiles to string ===
    personality_descriptions=[]
    for profile in profiles:
        try:
            personality_descriptions.append(profile.get("Psychological and Cognitive", {}).get("Personality/Big Five Traits", {}).get("General Big Five Description", ""))
        except:
            pass

    personality_descriptions = [desc for desc in personality_descriptions if isinstance(desc, str) and desc.strip()]


    # === Unique Profiles ===

    print("----------------------------Unique Profiles----------------------------")
    unique_profiles = set(compact_profiles)
    originality = len(unique_profiles) / len(compact_profiles)
    print(f"Porcentaje de perfiles únicos: {originality * 100:.2f}%")


    # === Cosine Similarity (profile embeddings) ===
    model = SentenceTransformer('all-mpnet-base-v2')
    embeddings = model.encode(compact_profiles)
    similarity_matrix = cosine_similarity(embeddings)
    mean_similarity = np.mean(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)])
    print("----------------------------Embedding Similarity----------------------------")
    print(f"Similitud promedio entre perfiles: {mean_similarity}")

    upper_tri_indices = np.triu_indices_from(similarity_matrix, k=1)
    similarities = similarity_matrix[upper_tri_indices]

    #plt.hist(similarities, bins=30)
    #plt.title("Similiraty Distribution Between Profiles")
    #plt.xlabel("Cosine similarity")
    #plt.ylabel("Frequency")
    #plt.savefig(f"similarity_distribution_{model_name}.png")
    #print("Saving 'similarity_distribution.png'")


    # Find most similar and most dissimilar pairs
    most_similar_idx = np.argmax(similarities)
    most_similar_pair = (upper_tri_indices[0][most_similar_idx], upper_tri_indices[1][most_similar_idx])
    print(f"Most similar profiles: {most_similar_pair}, Similarity: {similarities[most_similar_idx]:.4f}")
    

    #print(f"Profile 1: {compact_profiles[most_similar_pair[0]]}")
    #print(f"Profile 2: {compact_profiles[most_similar_pair[1]]}")

    # Perfiles más diferentes
    most_dissimilar_idx = np.argmin(similarities)
    most_dissimilar_pair = (upper_tri_indices[0][most_dissimilar_idx], upper_tri_indices[1][most_dissimilar_idx])
    print(f"Most dissimilar profiles: {most_dissimilar_pair}, Similarity: {similarities[most_dissimilar_idx]:.4f}")
    #print(f"profile 1: {compact_profiles[most_dissimilar_pair[0]]}")
    #print(f"profile 2: {compact_profiles[most_dissimilar_pair[1]]}")


    # === Cosine Similarity (general description and Big Five description) ===
    embeddings_descriptions = model.encode(descriptions)
    similarity_matrix_descriptions = cosine_similarity(embeddings_descriptions)
    mean_similarity_descriptions  = np.mean(similarity_matrix_descriptions[np.triu_indices_from(similarity_matrix_descriptions , k=1)])
    print(f"Average similarity between general descriptions: {mean_similarity_descriptions:.4f}")



    upper_tri_indices = np.triu_indices_from(similarity_matrix_descriptions, k=1)
    similarities = similarity_matrix_descriptions[upper_tri_indices]

    plt.hist(similarities, bins=30)
    plt.title("Distribución de similitudes entre descripciones")
    plt.xlabel("Similitud coseno")
    plt.ylabel("Frecuencia")
    plt.savefig(f"similarity_distribution_descriptions_{model_name}.png")

    ##General  Big 5 descriptions

    embeddings_descriptions = model.encode(personality_descriptions)
    similarity_matrix_descriptions = cosine_similarity(embeddings_descriptions)
    mean_similarity_descriptions  = np.mean(similarity_matrix_descriptions[np.triu_indices_from(similarity_matrix_descriptions , k=1)])
    print(f"Average similarity between general personalities: {mean_similarity_descriptions }")




    # === Distinct-1 (Lexical Diversity) ===

    distinct_1 = calculate_distinct_1(compact_profiles)
    distinct_1_description = calculate_distinct_1(descriptions)
    distinct_1_personality = calculate_distinct_1(personality_descriptions)
    print("----------------------------Lexic Diversity----------------------------")
    print(f"Distinct-1 (lexical diversity) - profiles: {distinct_1:.2f}")
    print(f"Distinct-1 (lexical diversity) - general descriptions: {distinct_1_description:.2f}")
    print(f"Distinct-1 (lexical diversity) - personality descriptions: {distinct_1_personality:.2f}")



    # === Jaccard Similarity per Attribute ===


    jaccard_results = jaccard_similarity_per_attribute(profiles)
    print("----------------------------Jaccard Similarity----------------------------")
    print("Average Jaccard similarity per attribute:")
    for attr, similarity in jaccard_results.items():
        print(f"{attr}: {similarity:.2f}")


    # === Global Entropy ===

    global_entropy = calculate_global_entropy(profiles)
    print(f"Global entropy of attribute values: {global_entropy:.2f}")


    #Closing standard output
    sys.stdout.close()
    sys.stdout = sys.__stdout__

